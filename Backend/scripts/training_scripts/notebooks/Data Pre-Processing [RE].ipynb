{"cells":[{"cell_type":"markdown","metadata":{"id":"VfXTcXRTelLA"},"source":["### Data Pre Processing\n","- Cleaning labelled data and conveting to DocBin Spacy Objects for model traning\n","- Annotating relations in Train, Test, Evaluation data"]},{"cell_type":"markdown","metadata":{"id":"8AfGKrPSglzj"},"source":["#### Note\n","- Upload Train, Test & Eval spacy datasets\n","- Annotation data file [.txt | .json]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmdXj9g_edKl"},"outputs":[],"source":["# Install necessary libraries\n","!pip install -U spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujSiMXtWfwZb"},"outputs":[],"source":["# Import necessary libraries\n","import json\n","import typer\n","import spacy\n","import re\n","from pathlib import Path\n","\n","from spacy.tokens import Span, DocBin, Doc\n","from spacy.vocab import Vocab\n","from wasabi import Printer\n","from spacy.tokenizer import Tokenizer\n","from spacy.lang.en import English\n","from spacy.util import compile_infix_regex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zw6xIin5gFqy"},"outputs":[],"source":["# Intialize blank spacy pipeline\n","nlp = spacy.blank(\"en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJJuqaAggL6-"},"outputs":[],"source":["# Labels used for annotation\n","SYMM_LABELS = [\"Binds\"]\n","MAP_LABELS = {\n","    \"ACCESS_USING\": \"ACCESS_USING\",\n","    \"INPUT\": \"INPUT\",\n","    \"NOTIFY\": \"NOTIFY\",\n","    \"AUTHENTICATION\": \"AUTHENTICATION\"\n","}\n","\n","msg = Printer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ve5p1eXqgbVN"},"outputs":[],"source":["# File Paths for data loading\n","annotated_data = \"/content/annotated_realtions.txt\"\n","train_file='/content/relations_training.spacy'\n","test_file='/content/relations_test.spacy'\n","dev_file='/content/relations_dev.spacy'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yc6v3DP9heFT"},"outputs":[],"source":["def annotator(json_data: Path, output_file: Path):\n","    \"\"\"Creating the corpus from annotations.\"\"\"\n","    Doc.set_extension(\"rel\", default={},force=True)\n","    vocab = Vocab()\n","\n","    docs = {\"train\": [], \"dev\": [], \"test\": [], \"total\": []}\n","    ids = {\"train\": set(), \"dev\": set(), \"test\": set(), \"total\":set()}\n","    count_all = {\"train\": 0, \"dev\": 0, \"test\": 0,\"total\": 0}\n","    count_pos = {\"train\": 0, \"dev\": 0, \"test\": 0,\"total\": 0}\n","\n","    with open(json_data, encoding=\"utf8\") as jsonfile:\n","        file = json.load(jsonfile)\n","        for example in file:\n","            span_starts = set()\n","            neg = 0\n","            pos = 0\n","                    # Parse the tokens\n","            tokens=nlp(example[\"document\"])    \n","\n","            spaces=[]\n","            spaces = [True if tok.whitespace_ else False for tok in tokens]\n","            words = [t.text for t in tokens]\n","            doc = Doc(nlp.vocab, words=words, spaces=spaces)\n","\n","\n","            # Parse the GGP entities\n","            spans = example[\"tokens\"]\n","            entities = []\n","            span_end_to_start = {}\n","            for span in spans:\n","                entity = doc.char_span(\n","                     span[\"start\"], span[\"end\"], label=span[\"entityLabel\"]\n","                 )\n","\n","\n","                span_end_to_start[span[\"token_start\"]] = span[\"token_start\"]\n","                #print(span_end_to_start)\n","                entities.append(entity)\n","                span_starts.add(span[\"token_start\"])\n","\n","            doc.ents = entities\n","\n","            # Parse the relations\n","            rels = {}\n","            for x1 in span_starts:\n","                for x2 in span_starts:\n","                    rels[(x1, x2)] = {}\n","                    #print(rels)\n","            relations = example[\"relations\"]\n","            #print(len(relations))\n","            for relation in relations:\n","                # the 'head' and 'child' annotations refer to the end token in the span\n","                # but we want the first token\n","                start = span_end_to_start[relation[\"head\"]]\n","                end = span_end_to_start[relation[\"child\"]]\n","                label = relation[\"relationLabel\"]\n","                #print(rels[(start, end)])\n","                #print(label)\n","                #label = MAP_LABELS[label]\n","                if label not in rels[(start, end)]:\n","                    rels[(start, end)][label] = 1.0\n","                    pos += 1\n","                    #print(pos)\n","                    #print(rels[(start, end)])\n","\n","            # The annotation is complete, so fill in zero's where the data is missing\n","            for x1 in span_starts:\n","                for x2 in span_starts:\n","                    for label in MAP_LABELS.values():\n","                        if label not in rels[(x1, x2)]:\n","                            neg += 1\n","                            rels[(x1, x2)][label] = 0.0\n","\n","                            #print(rels[(x1, x2)])\n","            doc._.rel = rels\n","            #print(doc._.rel)\n","\n","            # only keeping documents with at least 1 positive case\n","            if pos > 0:\n","                    docs[\"total\"].append(doc)\n","                    count_pos[\"total\"] += pos\n","                    count_all[\"total\"] += pos + neg\n","\n","                    \n","                    \n","    #print(len(docs[\"total\"]))\n","    output_path = \"/content/train-relation.spacy\"\n","    docbin = DocBin(docs=docs[\"total\"], store_user_data=True)\n","    docbin.to_disk(output_path)\n","    \n","    msg.info(\n","        f\"{len(docs['total'])} training sentences\"\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHVLTRxl8SPJ"},"outputs":[],"source":["annotator(annotated_data, train_file)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPpCqeeOpjtoF6fYm0kfvs4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
